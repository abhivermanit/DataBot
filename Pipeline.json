Downstream Pipeline :---

We use Apache Oozie to orchestrate our workflows, and this (show image if allowed) is a typical example of our pipeline visualized as a DAG (Directed Acyclic Graph).

The pipeline starts with two initialization steps:
generate-pipeline → patch-secrets-in-pipeline — these typically prepare configurations and secrets for the job.

Then, it kicks off the actual ETL flow with processMaintenanceDelta which is likely responsible for identifying or extracting delta (incremental) data.

Next, processMaintenanceTodoDetailsDump and MaintenanceTablesProcessing_1 are core aggregation or staging steps. They prepare multiple datasets.

Notice that after MaintenanceTablesProcessing_1, the pipeline fans out (parallelism) into multiple tasks:

processMaintenanceTicketEventsTables

processMaintenanceDashboardQueriesTables

processMaintenanceDashboardQueriesTodoAggrTables

processMaintenanceTicketsProjectsTodoAggr1Tables

This means data is being processed for different modules (Tickets, Dashboards, Projects) simultaneously.


"My main responsibility is to maintain, enhance, and troubleshoot this pipeline. Specifically, I:

Develop new nodes (jobs) when business wants new aggregations or metrics.

Optimize existing Spark/SQL jobs to improve performance.

Monitor this DAG daily via the Oozie UI and fix failures (data quality, resource issues, EMR errors).

Work closely with downstream consumers like the Dashboard or Analytics teams to ensure data accuracy."


Topic	Concepts to Cover
Data Pipelines	Batch vs Streaming, ETL vs ELT, Pipeline Design Patterns
DAG Concepts	What is DAG, why acyclic?, parallelism, scheduling
Workflow Orchestration	Oozie (since you're using it), Airflow basics, dependency management, retries
Big Data Tools	Spark (RDD, DataFrame, SparkSQL, Optimization, Actions, Transformations)
Distributed Systems	Partitioning, Shuffling, Fault Tolerance, Checkpointing
AWS & EMR	How EMR works, EMR + Spark setup, EMRFS, S3 integration
SQL	Window Functions, Joins, Aggregations, Common Table Expressions (CTE)
Data Modeling	Star Schema, Snowflake Schema, Fact and Dimension tables
Performance Tuning	Spark tuning, partitioning strategy, memory optimization
Monitoring	Logs, Metrics, handling failure scenarios, EMR & Oozie Monitoring


"I currently work as a Data Engineer responsible for building and managing distributed data pipelines.
Most of my work revolves around building Spark jobs orchestrated through Oozie, running on AWS EMR clusters. 
I ensure that data is ingested, transformed, and made available to downstream systems like dashboards and analytics platforms efficiently and reliably."

Pipeline Name	Maintenance Pipeline - Snowflake Full Load
Pipeline ID	Unique ID generated for every Oozie workflow execution
Cluster	The EMR cluster where Spark & other jobs run (uswest2-emr6-analytics02-gv-prod)
Artifact	The project/deployment we use (kh-analytics-dashboards - latest)
User	The ID under which the job runs (kha22)
Start / End	Pipeline ran on 29th March 2025, took around 4 hours 51 mins
Status	Job has successfully completed (SUCCEEDED)
Actions Completed	This pipeline finished 3 actions successfully
Navigation	We use DAG, Timeline, Actions, Metadata tabs to monitor progress & logs


✨ Steps Behind The Scene (Typical Flow)
Pipeline Trigger (Oozie Workflow + Cron)

Scheduled to run daily.

Starts with a full data pull or delta extraction.

Data Ingestion (Spark + EMR)

Spark reads data from S3 / source systems.

Handles massive volume using distributed processing.

Data Transformation

Business logic applied (joins, filters, aggregations).

Data is split into multiple output tables for dashboards, reports, and monitoring.

Load into Snowflake

Final transformed data is loaded into Snowflake using Snowflake Connector or Snowflake Spark JDBC.

Dashboard Ready

Data is now available for Tableau / Reporting tools.

Stakeholders use this for daily decision-making.


"If the pipeline fails, we check Spark UI, EMR logs, Oozie logs to identify the failure point. 
We fix the issue, whether it's data corruption, resource limits, or logic errors, and then we trigger a rerun either fully or partially depending on the failure point."


✅ Simple Story You Can Use
"Recently, I optimized one of the heavy transformations in this pipeline by reducing unnecessary shuffles and repartitioning, which reduced the runtime by 40 minutes. 
This not only improved performance but also made downstream Tableau dashboards available earlier every morning."



This pipeline is part of a data refresh system for Kitty Hawk Maintenance analytics. It does two big jobs:

- Loads & processes data from Snowflake by running a series of Snowflake SQL scripts (via SqlRunner).

- Refreshes Tableau dashboards automatically after the data is ready.


Script of the pipeline 

1. Pipeline Metadata & Constants :-----

- pipelineName, pipelineDescription — Just info about what this pipeline is.

- snowflakeScriptRunnerClass — Points to the Java class that will execute Snowflake SQL scripts.

- tableauRefresh — Points to the Java class that triggers Tableau API refresh.


2. List of Tableau Dashboards

val khMaintenanceTableauDashboards = List(...)


3. 




---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


spark pipelines 

Data Ingest :: Analytics Utility :: Warehouse to Snowflake - pipeline to move and transform data, likely from a warehouse into Snowflake. 

{
  "files": [],
  "configuration": {
    "njl.driver.argument.queue": "POI.GEMINI.NONCASSANDRA.NONPRIORITY",
    "spark.executor.memory": "18971M"
  },
  "metadata": {},
  "ignoreClusterConfiguration": false,
  "processContext": "processcontext=POI.poi_offline_exports-dev.GEMINI.NONCASSANDRA.NONPRIORITY/uniqueId/Snowflake",
  "timeout": 12,
  "email": {
    "usernames": [
      "david_avon",
      "dataops_bot"
    ],
    "groups": []
  }
}


"spark.executor.memory"	Sets the amount of memory each Spark executor gets (here, 18,971 MB or ~18.5 GB).





























