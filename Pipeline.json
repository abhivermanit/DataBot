Downstream Pipeline :---

We use Apache Oozie to orchestrate our workflows, and this (show image if allowed) is a typical example of our pipeline visualized as a DAG (Directed Acyclic Graph).

The pipeline starts with two initialization steps:
generate-pipeline → patch-secrets-in-pipeline — these typically prepare configurations and secrets for the job.

Then, it kicks off the actual ETL flow with processMaintenanceDelta which is likely responsible for identifying or extracting delta (incremental) data.

Next, processMaintenanceTodoDetailsDump and MaintenanceTablesProcessing_1 are core aggregation or staging steps. They prepare multiple datasets.

Notice that after MaintenanceTablesProcessing_1, the pipeline fans out (parallelism) into multiple tasks:

processMaintenanceTicketEventsTables

processMaintenanceDashboardQueriesTables

processMaintenanceDashboardQueriesTodoAggrTables

processMaintenanceTicketsProjectsTodoAggr1Tables

This means data is being processed for different modules (Tickets, Dashboards, Projects) simultaneously.


"My main responsibility is to maintain, enhance, and troubleshoot this pipeline. Specifically, I:

Develop new nodes (jobs) when business wants new aggregations or metrics.

Optimize existing Spark/SQL jobs to improve performance.

Monitor this DAG daily via the Oozie UI and fix failures (data quality, resource issues, EMR errors).

Work closely with downstream consumers like the Dashboard or Analytics teams to ensure data accuracy."


Topic	Concepts to Cover
Data Pipelines	Batch vs Streaming, ETL vs ELT, Pipeline Design Patterns
DAG Concepts	What is DAG, why acyclic?, parallelism, scheduling
Workflow Orchestration	Oozie (since you're using it), Airflow basics, dependency management, retries
Big Data Tools	Spark (RDD, DataFrame, SparkSQL, Optimization, Actions, Transformations)
Distributed Systems	Partitioning, Shuffling, Fault Tolerance, Checkpointing
AWS & EMR	How EMR works, EMR + Spark setup, EMRFS, S3 integration
SQL	Window Functions, Joins, Aggregations, Common Table Expressions (CTE)
Data Modeling	Star Schema, Snowflake Schema, Fact and Dimension tables
Performance Tuning	Spark tuning, partitioning strategy, memory optimization
Monitoring	Logs, Metrics, handling failure scenarios, EMR & Oozie Monitoring


"I currently work as a Data Engineer responsible for building and managing distributed data pipelines.
Most of my work revolves around building Spark jobs orchestrated through Oozie, running on AWS EMR clusters. 
I ensure that data is ingested, transformed, and made available to downstream systems like dashboards and analytics platforms efficiently and reliably."

Pipeline Name	Maintenance Pipeline - Snowflake Full Load
Pipeline ID	Unique ID generated for every Oozie workflow execution
Cluster	The EMR cluster where Spark & other jobs run (uswest2-emr6-analytics02-gv-prod)
Artifact	The project/deployment we use (kh-analytics-dashboards - latest)
User	The ID under which the job runs (kha22)
Start / End	Pipeline ran on 29th March 2025, took around 4 hours 51 mins
Status	Job has successfully completed (SUCCEEDED)
Actions Completed	This pipeline finished 3 actions successfully
Navigation	We use DAG, Timeline, Actions, Metadata tabs to monitor progress & logs


✨ Steps Behind The Scene (Typical Flow)
Pipeline Trigger (Oozie Workflow + Cron)

Scheduled to run daily.

Starts with a full data pull or delta extraction.

Data Ingestion (Spark + EMR)

Spark reads data from S3 / source systems.

Handles massive volume using distributed processing.

Data Transformation

Business logic applied (joins, filters, aggregations).

Data is split into multiple output tables for dashboards, reports, and monitoring.

Load into Snowflake

Final transformed data is loaded into Snowflake using Snowflake Connector or Snowflake Spark JDBC.

Dashboard Ready

Data is now available for Tableau / Reporting tools.

Stakeholders use this for daily decision-making.


"If the pipeline fails, we check Spark UI, EMR logs, Oozie logs to identify the failure point. 
We fix the issue, whether it's data corruption, resource limits, or logic errors, and then we trigger a rerun either fully or partially depending on the failure point."


✅ Simple Story You Can Use
"Recently, I optimized one of the heavy transformations in this pipeline by reducing unnecessary shuffles and repartitioning, which reduced the runtime by 40 minutes. 
This not only improved performance but also made downstream Tableau dashboards available earlier every morning."



This pipeline is part of a data refresh system for Kitty Hawk Maintenance analytics. It does two big jobs:

- Loads & processes data from Snowflake by running a series of Snowflake SQL scripts (via SqlRunner).

- Refreshes Tableau dashboards automatically after the data is ready.


Script of the pipeline 

1. Pipeline Metadata & Constants :-----

- pipelineName, pipelineDescription — Just info about what this pipeline is.

- snowflakeScriptRunnerClass — Points to the Java class that will execute Snowflake SQL scripts.

- tableauRefresh — Points to the Java class that triggers Tableau API refresh.


2. List of Tableau Dashboards

val khMaintenanceTableauDashboards = List(...)


3. 




---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


SPARK PIPELINE :-

how large companies build and manage their data infrastructure using tools that help automate, schedule, and run data workflows at scale.

An Enterprise Data Platform is a system 

Step 1 - Cloud storage (S3, ADLS)

Step 2 - - Workflow orchestration (Oozie, Airflow)

Step 3 - Data processing engines (Spark, Flink, Hive)

Step 4 - Data warehouses (Snowflake, BigQuery, Redshift)



Pipeline Configuration :-

Spectrum Compliance Snowflake Pipeline

- The pipeline config (JSON or XML for Oozie) is stored in a Git repo.

- This configuration sets up how a Spark job will be submitted and executed on a Hadoop cluster with the below points

1. Resource allocation (memory, cores, instances)

2. Job behavior (timeouts, dynamic allocation)

3. Cluster scheduling (queue name, fairness)

4. Snowflake export likely happens at the end via a connector or JDBC


val sparkConfigs = Map(
    "spark.executor.cores" -> "4",
    "spark.scheduler.mode" -> "FAIR",
    "spark.executor.heartbeatInterval" -> "60s",
    "spark.app.name" -> "processcontext=ANALYTICS.kh-build-ops-analytics-prod.AGGREGATES_PROD usercontext=kha22",
    "spark.yarn.am.memory" -> "48g",
    "spark.driver.memory" -> "48g",
    "spark.driver.userClassPathFirst" -> "false",
    "spark.dynamicAllocation.maxExecutors" -> "10",
    "spark.network.timeout" -> "3600s",
    "spark.rpc.askTimeout" -> "3600s",
    "spark.rpc.message.maxSize" -> "850",
    "spark.yarn.am.waitTime" -> "3600s",
    "spark.dynamicAllocation.minExecutors" -> "1",
    "spark.executor.userClassPathFirst" -> "false",
    "spark.yarn.queue" -> "ANALYTICS.AGGREGATES_PROD.AGGREGATES_PROD",
    "spark.executor.instances" -> "5",
    "spark.storage.memoryFraction" -> "0.05",
    "spark.executor.memory" -> "16g",
    "spark.kryoserializer.buffer.max" -> "1024m",
    "spark.shuffle.service.enabled" -> "true",
    "spark.yarn.driver.memoryOverhead" -> "4096",
    "spark.yarn.am.memoryOverhead" -> "4096",
    "spark.yarn.executor.memoryOverhead" -> "1600",
    "spark.driver.extraClassPath" -> "../config/hadoop-alt:../config/hive"
  )

1. Core Resource Settings

"spark.executor.cores" -> "4"
"spark.executor.memory" -> "16g"
"spark.executor.instances" -> "5"
"spark.driver.memory" -> "48g"

Each executor gets 4 CPU cores.
More cores = more parallel tasks per executor.
Each executor gets 16 GB RAM (for processing).
5 executors will be launched.

Total Spark resources: 5 executors × 4 cores = 20 cores, and 5 × 16 GB = 80 GB of executor memory (plus overhead).

The driver (master program) and ApplicationMaster get 48 GB RAM — quite high, meaning your pipeline is doing substantial orchestration or data movement.


2. Execution Environment & Optimization

"spark.dynamicAllocation.minExecutors" -> "1"
"spark.dynamicAllocation.maxExecutors" -> "10"

Spark will scale executors between 1 and 10 based on the job load.

Works only if dynamic allocation is enabled 

"spark.shuffle.service.enabled" -> "true"

Required for dynamic allocation to work (because shuffled data may be reused between executors).

"spark.storage.memoryFraction" -> "0.05"

Only 5% of executor memory is used for caching/storage. You’re optimizing more for computation than caching.

Executor memory in Spark is divided between:
Storage memory → used for caching (e.g., .cache(), .persist() results).
Execution memory → used for computation (e.g., joins, shuffles, aggregations).

Optimization - Not caching large reused DataFrames means missed opportunity for faster pipelines.
Performance	Without caching - Spark recomputes data multiple times, increasing job duration.

Caching - It allows Spark to keep data in memory rather than recomputing it or reading it again from disk or a remote source (like Snowflake or HDFS).

When you re-use the same DataFrame/RDD multiple times, Spark will by default recompute it each time unless you tell it to cache or persist.

Persist - Storage Level	MEMORY_AND_DISK (default)	Customizable (e.g., MEMORY_ONLY, DISK_ONLY)
Caching - Usage Simplicity	Shorthand for common case	Flexible with more options


Why Spark Recomputes Without cache()? 

- Spark uses lazy evaluation, so it doesn’t immediately execute this line.
It builds a logical execution plan (a DAG) — a graph of operations to be executed only when an action is triggered (like .count(), .collect(), .write()).

{val df = spark.read.parquet("/big/path")
val filtered = df.filter("status = 'active'").select("user_id")

val result1 = filtered.groupBy("user_id").count()
val result2 = filtered.where("user_id > 1000").count()} 

For result1, Spark builds a DAG that goes:
Read Parquet → Filter → Select → GroupBy

For result2, Spark builds another DAG:
Read Parquet → Filter → Select → Where

Concept                        Behavior
--------------   --------------------------------------
Spark is lazy	It doesn't execute until an action is called
Without cache()	Every action re-triggers the full upstream transformations
With cache()	Spark stores intermediate results in memory for reuse



3. Timeouts & Performance Tuning

"spark.executor.heartbeatInterval" -> "60s"
"spark.network.timeout" -> "3600s"
"spark.rpc.askTimeout" -> "3600s"
"spark.rpc.message.maxSize" -> "850"

These configs handle communication timeouts between driver and executors.
Set to high values to prevent false timeouts during long stages or large shuffles.


- Performance tuning is the process of optimizing a Spark application to

what will perfomance tuning solve:- 
1. Run faster

2. Use fewer resources (memory, CPU, disk, network)

3. Be more stable and scalable


pointers to look at to tune:- 

- Cluster resources (executors, memory, cores)

- Spark configs (timeouts, shuffles, serialization)

- Data handling (partitioning, caching, joins)

- Job logic (code structure, transformations)


Level	                                    Focus Areas
System-level tuning	        Spark config parameters (like memory, timeouts)
Application-level tuning	How your job logic and DataFrames are written


"spark.executor.heartbeatInterval" -> "60s"
Each executor sends a heartbeat to the driver every 60 seconds.
If executors are doing long GC or complex tasks, they might not heartbeat in time, and the driver will think they're dead.
Increase this (e.g., to 60s–120s) for heavy workloads or long tasks.

"spark.network.timeout" -> "3600s"
For long-running jobs, this prevents Spark from killing healthy but busy nodes.

"spark.rpc.message.maxSize" -> "850"
Limits the size of messages sent between executors and the driver.
If you're transferring big broadcast variables or shuffle blocks, too small a size causes failures.
850 mb is a huge size


How tuninng is done in real life :-
| Step | What to Do                    | Tools/How                                         |
| ---- | ----------------------------- | ------------------------------------------------- |
| 1    | Understand workload           | DAG + Spark UI + Metrics                          |
| 2    | Tune memory & executors       | `executor.memory`, `executor.cores`, `instances`  |
| 3    | Fix skew & shuffles           | Partitioning, avoid wide joins                    |
| 4    | Tune caching                  | `cache()`, `persist()` + `storage.memoryFraction` |
| 5    | Tune timeouts & communication | Like the ones you asked about                     |
| 6    | Benchmark & monitor           | Spark History Server, Ganglia, CloudWatch, etc.   |



| Config                         | Typical Range             | Purpose                            |
| ------------------------------ | ------------------------- | ---------------------------------- |
| `spark.executor.memory`        | 4–32g                     | Total memory per executor          |
| `spark.executor.cores`         | 2–5                       | Cores per executor                 |
| `spark.executor.instances`     | Depends on cluster        | How many executors                 |
| `spark.sql.shuffle.partitions` | 100–500                   | Tune for join/shuffle optimization |
| `spark.memory.fraction`        | 0.6–0.8                   | For execution + storage            |
| `spark.rpc.*` / `network.*`    | Longer timeouts if needed | For stability on large data        |




Clusters and configurations :- 

Amazon EMR (Elastic MapReduce)
Region: us-west-2
Cluster: emr-5.36.1
Cluster Name: uswest2-emr5-dataingest05-gv-prod

We run our Spark pipelines on Amazon EMR — specifically EMR version 5.36.1 — in the us-west-2 region. 
It’s a production-grade cluster used for high-throughput data ingestion jobs, such as loading and transforming data into Snowflake and other downstream systems.

Amazon EMR is a managed big data platform on AWS that lets you run frameworks like Spark, Hadoop, Hive on EC2 clusters without managing infrastructure manually.

EC2 - Elastic Compute Cloud 

Hadoop provides the underlying YARN cluster manager and HDFS (storage) for Spark and Hive jobs.

Spark is your core computation engine — used for reading data (e.g. from S3), transforming it, joining, aggregating, and writing it back (e.g. to Snowflake).
This is a stable, pre-Spark 3.x version, where you have to tune joins, shuffles, and memory more manually. Spark 3.x has adaptive query execution (AQE), but 2.4.8 does not.

Scala is the programming language Spark is written in, and also the primary language used to write Spark applications in enterprise settings.
Your code must be compiled with Scala 2.11 to be compatible with Spark 2.4.8. Libraries and dependencies must also match the Scala version.
We developed our Spark jobs in Scala 2.11 to align with the Spark 2.4.8 runtime on EMR. This gave us tight integration with Spark’s RDD and Dataset APIs

| Component  | Version | Purpose                   | What to Say in Interview                                                          |
| ---------- | ------- | ------------------------- | --------------------------------------------------------------------------------- |
| **EMR**    | 5.36.1  | AWS-managed Spark cluster | “EMR handled provisioning, scaling, and monitoring of our Spark pipelines.”       |
| **Hadoop** | 2.10.1  | YARN resource management  | “YARN was used to allocate executor memory, cores, and monitor tasks.”            |
| **Spark**  | 2.4.8   | Compute engine            | “We built ETL jobs in Spark, tuned joins, partitions, and memory usage manually.” |
| **Hive**   | 2.3.9   | Metastore and SQL engine  | “Used for table schemas and some SQL-based batch queries.”                        |
| **Scala**  | 2.11    | Language for Spark code   | “Our codebase was in Scala 2.11 for compatibility with Spark 2.4.”                |

| Topic                  | What You Can Say                                                            |
| ---------------------- | --------------------------------------------------------------------------- |
| **Cluster Info**       | EMR 5.36.1 in `us-west-2` for production-grade Spark jobs                   |
| **Spark Version**      | Spark 2.4.8 with custom tuning (executors, memory, timeouts)                |
| **Job Type**           | ETL ingestion pipelines into Snowflake with scheduling via Oozie/Airflow    |
| **Performance Tuning** | Managed GC stalls, executor losses, shuffle joins using Spark configs       |
| **Monitoring**         | Used Spark UI, CloudWatch, and S3 logs for deep debugging and optimizations |

Most of our jobs are large-scale ETL pipelines — reading from S3, doing enrichment and joins, then writing to Snowflake.
We handle both batch and scheduled near-real-time jobs orchestrated via Oozie or Airflow.

Some common fixes in the project :- 

- Some jobs read 200–500 GB of raw data per run. One common issue was executor timeouts during wide joins — we resolved this by increasing network timeouts and tuning the broadcast threshold.
- Our pipelines were built in Spark 2.4.8, which meant we manually tuned shuffle partitions and caching logic.
I worked with transformations like groupBy, join, withColumn, and optimized wide joins by leveraging broadcast joins and partitioning logic.
- Your code must be compiled with Scala 2.11 to be compatible with Spark 2.4.8. Libraries and dependencies must also match the Scala version.
- Some pipelines were impacted by skewed joins where one key had significantly more data. 
I resolved this by detecting skew from the stage-level metrics in Spark UI and implemented salting + repartitioning. techniques during joins
- In a pipeline that calculated aggregates over multiple partitions and involved window functions, the same filtered DataFrame was used in multiple actions.
Without caching, it was recomputed for each action. I used df.cache() strategically after expensive transformations to avoid recomputation and reduced job time from ~45 minutes to 20 minutes.
- For pipelines writing to Snowflake, we noticed high latency and failures during writes when partition sizes were inconsistent.
We tuned the batch size in spark-snowflake connector and used coalesce() before writes to reduce the number of partitions, improving write throughput.
- We also disabled truncate_table in overwrite mode to avoid full table reloads unless necessary.
- I noticed large shuffle file sizes (~2–3 GB). I increased spark.shuffle.file.buffer and spark.reducer.maxSizeInFlight to handle larger blocks, and ensured shuffle spill compression was enabled.



















